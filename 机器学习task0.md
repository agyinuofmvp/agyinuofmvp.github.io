# 机器学习  
## 什么是机器学习？  
通俗来讲，就是让计算机通过已有数据进行训练学习，而无需显示编程的技术。包括多种算法和方法，线性回归，决策树，支持向量机，K近邻等
## Supervised Learning or Unsupervised Learning  
监督学习与无监督学习都是机器学习的主要方法，主要区别在于数据标签的有无以及目标的不同

|                 | Supervised Learning                                          | Unsupervised Learning                                        |
| --------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 数据标签(label) | 使用带标签的数据集进行训练。每条输入数据(input x)都对应一个已知的输出(output y) | 使用无标签的数据集进行训练，模型不知道每条数据的正确答案     |
| 目标(target)    | 通过已有数据训练模型，使得模型能够准确预测或分类未标记的数据 | 在数据中发现潜在的模式、结构或规律                           |
| 常见任务        | 分类：预测离散类别标签；回归：预测连续数值输出               | 聚类：将相似的数据点分为同一组；降维：将高维数据映射到低维空间，以便于可视化和处理 |
| 算法类型        | 线性回归，支持向量机(SVM)，决策树，神经网络等                | K均值聚类，层次聚类，主成分分析(PCA)等                       |

## 机器学习 or 深度学习  
包含与被包含的关系，深度学习是机器学习的一个子领域，依赖于人工神经网络。通过构建神经网络模型，从大量数据中抽象出复杂的模式，通常适合处理图像，语音，文本等复杂，非结构化数据  
## 数学基础
### 偏导数
用于计算多变量函数中一个变量变化时，函数值的变化率。  
举例：在神经网络中，通过计算损失函数对模型参数(如权重，偏差)的偏导数，来调整这些参数，降低模型的误差。对于一个损失函数L(w,b),权重$w=w-α\frac{\partial L}{\partial w}$,偏置量$b=b-α\frac{\partial L}{\partial b}$,其中α表示学习率  

### 链式法则
一种求复合函数导数的方法，当模型中计算过程是多个函数嵌套时，链式法则可以帮助我们逐层计算这些嵌套函数的导数  
举例：在神经网络中，链式法则是反向传播的核心，从输出层开始，逐层向前计算梯度，从而高效地更新每一层的参数  
  ![backward](.\backward.png)

### 梯度
梯度是一个向量，表示函数在某一点的变化方向和速率，梯度指向损失函数增加最快的方向  
举例：梯度下降法--一种常用的优化算法，用于最小化损失函数，沿着梯度的反方向调整参数，便可逐步逼近最优解  
### 矩阵
用于表示和操作多维数据，尤其是神经网络中的输入，权重和输出，矩阵运算可以快速处理大量数据  
举例：向前传播--神经网络中的每一层可以看作是一次矩阵乘法操作，对于$y=wx$来说，假设y是一个2 x 3矩阵，x是一个1 x 3的矩阵，那么w就是一个2 x 1的矩阵  
要知道，每一个模型都不只是单一的数学概念的应用，往往多个数学概念同时应用，可以快速并方便的处理大量数据  

## 深度学习--神经网络模型 
**基本组成**：模仿了生物神经系统的神经元，由输入层，隐藏层，输出层组成，层与层之间通过权重相连  
**输入层**：接收输入数据的第一层，每个神经元对应一个输入特征，输入层的节点数等于输入数据的维度  
**隐藏层**：包含多个神经元，每个神经元与前一层的所有神经元相连，通过权重和激活函数提取数据的特征和模式，可以有不止一个隐藏层  
**输出层**：用于模型的输出，节点数量取决于任务类型，比如分类任务输出层节点数等于类别数，回归任务一般只有一个节点，用于输出一个连续值  
**神经元**：神经网络的基本单元，每个神经元执行以下操作  
加权求和：每个输入特征乘以相应权重，同时添加偏置量，$z=w_1x_1+……+w_nx_n+b$

激活函数：进行非线性变换，使模型能够学习复杂模式，$a=σ(x)$

**权重**：连接神经元的参数，表示输入的影响力，决定了对神经元输出的贡献  
**偏置项**：帮助调整模型的输出而步依赖于输入特征，可以使得模型的激活函数有一个灵活的偏移量  
**激活函数**：用于引入非线性，使神经网络能够处理复杂的模式和关系，举一个简单的例子来理解：  
假设每一层神经元都不使用激活函数，那么$y=w_2(w_1x+b_1)+b_2$最终都可以化简成$y=wx+b$

这导致神经网络模型只是一个简单的线性模型，无法处理更复杂的模式和关系   
常见的激活函数：  
1.Sigmoid函数--$σ(x)=\frac{1}{1+e^{-x}}$,输出值(0,1)，简单但在输入非常大或小时，梯度接近于0，导致训练变慢  
2.Tanh函数--$tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$,输出值(-1,1),更加平衡，输出更接近于0，梯度消失问题较轻  
3.ReLU函数--$f(x)=max(0,x)$,输出值(0,+$\infty$),计算简单，收敛速度快，能有效解决梯度消失问题，但当输入为负时，输出为0，可能导致部分"神经元"死亡(Dying ReLU问题)  
4.ELU函数--$当x>0,f(x)=x;当x<=0,f(x)=α(e^x-1)$,负区域输出非零值，减少梯度消失问题，能改善模型性能，但计算复杂度较高  

## 机器学习中的数据处理  
### 1.数据清洗  
数据通常包含噪声，缺失值或异常值，直接使用可能会导致模型表现不佳。  
缺失值处理--数量较少，直接删；数量较多，可以使用均值，中位数，众数等填充缺失值  
异常值处理--使用统计方法或可视化工具(如箱线图)检测，然后删除或替换异常值  
### 2.特征工程  
将原始数据转换为适合模型的输入，包括特征提取，特征转换，特征组合以及特征选择  
### 3.数据标准化与归一化  
特征数据往往有不同的尺度，特征的取值范围不同可能会导致某些特征在训练中被过分重视或忽略
**标准化**：将特征转换成均值为0，方差为1的标准正态分布，$x=\frac{x-μ}{σ}$,适用于依赖特征间距离的算法(如SVM,KNN,线性回归)  
**归一化**：将数据缩放到[0,1]或[-1,1]的范围，$x=\frac{x-min(x)}{max(x)-min(x)}$，适用于距离度量算法(如：KNN,K-means)和神经网络等  
### 4.数据集划分  
**训练集(Training Set)**：用于训练模型  
**验证集(Validation Set)**:用于模型的超参数调整和模型选择，避免模型过拟合  
**测试集(Test Set)**:用于评估模型在未见过的数据上的性能  
### 5.数据降维  
在高维数据中，许多特征是不需要的，降维能帮助减少特征数量，加快训练速度，并减少过拟合的风险  
**主成分分析**：一种线性降维方法，通过将原始特征转换到新的坐标系上，能够找到解释最多方差的主成分  
**线性判别分析**：主要用于分类问题，通过最大化类间方差和最小化类内方差来降维
### 6.类别数据处理  
当数据是类别型时(如：性别，颜色等),需要转换维模型能够处理的数值型数据，解决方法有标签编码和独热编码(遇到了再具体研究)  
### 7.数据增强  
在数据不足的情况下，数据增强可以通过对现有数据的改变或扩展，生成新的训练数据。常见于图像、文本和时间序列数据处理  
### 8.特征缩放  
不同特征的取值范围不同，可能影响算法的训练效果。例如，特征 A 的范围是 [1, 100]，特征 B 的范围是 [0.001, 0.01]，在没有缩放的情况下，特征 A 的权重可能会被模型过分重视。  
每一步数据处理都需要根据需求，现在只了解了一部分，后续学习越来越深入再加深理解